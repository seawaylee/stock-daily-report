"""
[Module 7] Economic Monitor (Source: EastMoney 7x24)
Generates:
1. Economic Brief Prompt (Daily Top 10 + Targeted Sentiment)
2. Weekly Core Summary (Weekly Top 10 + Targeted Sentiment)
Source: EastMoney (ä¸œæ–¹è´¢å¯Œ) 7x24 Global Live Feed
Features: 24h/7d Deep Fetch + Sector-Level Sentiment + Dynamic Footer Summary
"""
import requests
import json
from datetime import datetime, timedelta
import os
import re
import time
import sys

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from common.image_generator import generate_image_from_text

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_raw_image_prompt_daily(date_disp):
    """Generate raw English prompt for Daily News cover"""
    prompt = (
        f"Hand-drawn financial infographic poster, China A-share market news, 24h summary {date_disp}. "
        f"Style: Warm cream paper texture (#F5E6C8), vintage notebook aesthetic, handwritten Chinese fonts. "
        f"Visual elements: Newspaper clippings, red upward arrows for bullish news, green downward arrows for bearish news. "
        f"Layout: 9:16 vertical, title 'Aè‚¡24å°æ—¶é‡è¦èµ„è®¯'. "
        f"Atmosphere: Professional, informative, vintage media style. "
        f"--ar 9:16 --style raw --v 6"
    )
    return prompt

def get_raw_image_prompt_weekly():
    """Generate raw English prompt for Weekly News cover"""
    prompt = (
        f"Hand-drawn financial infographic poster, China A-share weekly summary. "
        f"Style: Warm cream paper texture (#F5E6C8), vintage notebook aesthetic, handwritten Chinese fonts. "
        f"Visual elements: Weekly calendar pages, stacked documents, trend lines. "
        f"Layout: 9:16 vertical, title 'Aè‚¡æœ¬å‘¨é‡è¦å›é¡¾'. "
        f"Atmosphere: Comprehensive, summary style. "
        f"--ar 9:16 --style raw --v 6"
    )
    return prompt

def generate_podcast_text(news_list, is_weekly=False):
    """Generate podcast script from news list"""
    date_str = datetime.now().strftime("%mæœˆ%dæ—¥")
    title = "æœ¬å‘¨Aè‚¡æ ¸å¿ƒå›é¡¾" if is_weekly else f"{date_str} Aè‚¡24å°æ—¶è¦é—»ç²¾é€‰"

    text = f"""å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯é‡åŒ–å°ä¸‡ã€‚ç°åœ¨ä¸ºæ‚¨æ’­æŠ¥{title}ã€‚

"""

    count = 1
    for item in news_list:
        # Item format: "[10:30]ã€åˆ©å¤šÂ·åŠå¯¼ä½“ã€‘ æ ‡é¢˜..."
        # Extract title part
        try:
            # Remove timestamp and tags for reading
            # Simple regex to remove [...]...ã€‘
            clean_content = re.sub(r'^\[.*?\]ã€.*?ã€‘\s*', '', item)

            # If extraction fails, use original
            content = clean_content if clean_content else item

            text += f"ç¬¬{count}æ¡ï¼š{content}ã€‚\n"
            count += 1
        except:
            continue

    text += """
ä»¥ä¸Šå°±æ˜¯ä»Šå¤©çš„é‡ç‚¹èµ„è®¯ï¼Œæ„Ÿè°¢æ”¶å¬ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ã€‚
"""
    return text

def fetch_eastmoney_data(target_window_hours=24):
    """Fetch 7x24 news from EastMoney until target window covered"""
    base_url = "https://newsapi.eastmoney.com/kuaixun/v1/getlist_102_ajaxResult_50_{}_.html"
    
    all_data = []
    cutoff_time = datetime.now() - timedelta(hours=target_window_hours)
    
    max_pages = 150 if target_window_hours > 24 else 20
    
    for page in range(1, max_pages + 1):
        url = base_url.format(page)
        try:
            r = requests.get(url, headers=HEADERS, timeout=5)
            if r.status_code == 200:
                text = r.text
                if "var ajaxResult=" in text:
                    json_str = text.split("var ajaxResult=")[1].strip().rstrip(";")
                    data = json.loads(json_str)
                    items = data.get('LivesList', [])
                    
                    if not items: break
                        
                    for item in items:
                        showtime = item.get('showtime', '')
                        try:
                            item_dt = datetime.strptime(showtime, "%Y-%m-%d %H:%M:%S")
                        except:
                            continue
                            
                        if item_dt < cutoff_time:
                            # Reached limit
                            all_data.append({'time': item_dt, 'title': item.get('digest', ''), 'code': item.get('code', '')})
                            return all_data
                            
                        all_data.append({
                            'time': item_dt, 
                            'title': item.get('digest', ''),
                            'code': item.get('code', '')
                        })
                else:
                    break
            else:
                break
        except Exception as e:
            print(f"Error: {e}")
            break
            
        time.sleep(0.1)
        
    return all_data

def get_sentiment_and_target(text):
    """Analyze Sentiment AND Target Sector"""
    direction = ""
    bullish_kw = ['å¢é•¿', 'å¤§å¢', 'å€å¢', 'çªç ´', 'æ–°é«˜', 'è·æ‰¹', 'ä¸­æ ‡', 'å›è´­', 'å¢æŒ', 'åˆ†çº¢', 'åˆ©å¥½', 'è½åœ°', 'å°å‘', 'é€šè¿‡', 'å¤è‹', 'ä¸Šè°ƒ', 'ä¹°å…¥', 'å¢ä»“', 'è§£ç¦', 'ä¸Šå¸‚', 'IPO', 'å¤§æ¶¨', 'æ¶¨åœ']
    bearish_kw = ['ä¸‹è·Œ', 'å¤§è·Œ', 'æ–°ä½', 'äºæŸ', 'ç«‹æ¡ˆ', 'è°ƒæŸ¥', 'å¤„ç½š', 'è­¦ç¤º', 'é€€å¸‚', 'å‡æŒ', 'æŠ›å”®', 'ä¸‹è°ƒ', 'æ”¾ç¼“', 'èç¼©', 'è¿çº¦', 'æš´é›·', 'ç›‘ç®¡', 'çƒ‚å°¾']
    
    score = 0
    for k in bullish_kw:
        if k in text: score += 1
    for k in bearish_kw:
        if k in text: score -= 1
        
    if score > 0: direction = "åˆ©å¤š"
    if score < 0: direction = "åˆ©ç©º"
    
    # 2. Determine Target Sector
    target = ""
    sector_map = {
        'ç”µå­': ['èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'é›†æˆç”µè·¯', 'åä¸º', 'è‹¹æœ', 'æ‰‹æœº', 'æ¶ˆè´¹ç”µå­', 'é¢æ¿'],
        'AI': ['äººå·¥æ™ºèƒ½', 'AI', 'å¤§æ¨¡å‹', 'ç®—åŠ›', 'è‹±ä¼Ÿè¾¾', 'OpenAI', 'Sora'],
        'æ–°èƒ½æº': ['å…‰ä¼', 'ç”µæ± ', 'é”‚', 'å®å¾·æ—¶ä»£', 'å‚¨èƒ½', 'é£ç”µ'],
        'æ±½è½¦': ['æ±½è½¦', 'æ¯”äºšè¿ª', 'é—®ç•Œ', 'ç†æƒ³', 'ç‰¹æ–¯æ‹‰', 'è‡ªåŠ¨é©¾é©¶'],
        'åœ°äº§': ['æˆ¿åœ°äº§', 'æ¥¼å¸‚', 'ä¸‡ç§‘', 'ä¿åˆ©', 'æ’å¤§', 'é”€å”®é¢ç§¯', 'æ‹¿åœ°', 'ç‰©ä¸š'],
        'é‡‘è': ['é“¶è¡Œ', 'åˆ¸å•†', 'è¯åˆ¸', 'ä¿é™©', 'ç¤¾è', 'ä¿¡è´·', 'LPR', 'é™å‡†', 'é™æ¯', 'è´§å¸'],
        'åŒ»è¯': ['è¯', 'åŒ»ç–—', 'å™¨æ¢°', 'è·æ‰¹', 'ä¸´åºŠ'],
        'ç™½é…’': ['ç™½é…’', 'èŒ…å°', 'äº”ç²®æ¶²'],
        'ä½ç©º': ['ä½ç©ºç»æµ', 'é£è¡Œæ±½è½¦', 'æ— äººæœº'],
        'èˆªå¤©': ['èˆªå¤©', 'å«æ˜Ÿ', 'ç«ç®­'],
        'å®è§‚': ['GDP', 'CPI', 'PPI', 'PMI', 'å¤®è¡Œ', 'è´¢æ”¿éƒ¨', 'å‘æ”¹å§”', 'ç»Ÿè®¡å±€', 'è¿›å‡ºå£']
    }
    
    for sector, kws in sector_map.items():
        if any(k in text for k in kws):
            target = sector
            break
    
    # Fallback targets
    if not target:
        if 'å…¬å¸' in text or 'è‚¡ä»½' in text: target = "ä¸ªè‚¡"
        elif direction: target = "è¡Œä¸š"
            
    return direction, target

def clean_text_gentle(text):
    """
    Remove bureaucratic headers/dates but keep full semantic meaning.
    Do NOT truncate aggressively.
    """
    # 1. Clean Garbage Headers
    text = re.sub(r'^.*?[:ï¼š]', '', text) 
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'\(\d{6}\)', '', text)
    text = re.sub(r'æ®æŠ¥é“[ï¼Œ,]', '', text)
    text = re.sub(r'æ®.*?æ¶ˆæ¯[ï¼Œ,]', '', text)
    text = re.sub(r'æ¶ˆæ¯ç§°[ï¼Œ,]', '', text)
    text = re.sub(r'æ•°æ®æ˜¾ç¤º[ï¼Œ,]', '', text)
    text = re.sub(r'è®°è€…è·æ‚‰[ï¼Œ,]', '', text)
    text = re.sub(r'è´¢è”ç¤¾\d+æœˆ\d+æ—¥ç”µ', '', text)
    
    # Text specific fixes
    text = re.sub(r'\d+æœˆ\d+æ—¥[æ™šæ—©åˆ]?[ï¼Œ,]', '', text) 
    text = re.sub(r'\d+æœˆ\d+æ—¥', '', text) 
    
    # 2. Simplify Numbers (Gentle)
    text = text.replace("äº¿å…ƒ", "äº¿").replace("ä¸‡å…ƒ", "ä¸‡")
    text = text.replace("äººæ°‘å¸", "")
    text = text.replace("è‚¡ä»½æœ‰é™å…¬å¸", "")
    text = text.replace("æœ‰é™è´£ä»»å…¬å¸", "")
    
    # 3. Strip dangling brackets
    text = text.replace('ã€‘', '').replace('ã€', '')
    
    # 4. Strip
    final = text.strip()
    # Remove leading punctuation/garbage words
    final = re.sub(r'^[æ™šæ—©åˆæ—¥][ï¼Œ,]', '', final)
    final = final.lstrip('ï¼Œ,ã€‚.:')
    
    return final

def calculate_importance(title):
    """Score logic: Kill International, Kill Index, Boost A-Share"""
    score = 0
    text = title
    
    # 0. STRICT BLACKLIST (International / Noise)
    blacklist = [
        'ç¾è”å‚¨', 'çº³æ–¯è¾¾å…‹', 'é“ç¼æ–¯', 'æ ‡æ™®', 'æ‹œç™»', 'ç¾å…ƒ', 'æ¬§å…ƒ', 'æ—¥å…ƒ', 'è‹±é•‘', 'éŸ©å…ƒ',
        'æ¬§å¤®è¡Œ', 'WTI', 'å¸ƒä¼¦ç‰¹', 'æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 
        'è‹±å›½', 'å¾·å›½', 'æ³•å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'å°åº¦', 'è¶Šå—', 'å§”å†…ç‘æ‹‰', 'ä¼Šæœ—',
        'æ”¶ç›˜', 'å¼€ç›˜', 'æ—©ç›˜', 'åˆç›˜', 'å°¾ç›˜', 'ä¸‰å¤§æŒ‡æ•°', 'ä¸¤å¸‚', 'åŒ—å‘èµ„é‡‘', 'æˆäº¤é¢'
    ]
    
    # Allow logic
    has_china = any(kw in text for kw in ['ä¸­å›½', 'å¤®è¡Œ', 'Aè‚¡', 'ä¸­æ¦‚', 'å¯¹å', 'åˆ¶è£', 'é©»å'])
    
    for bad in blacklist:
        if bad in text and not has_china:
            return -100 
            
    if len(text) < 5: return -100

    # 1. Critical Policy
    critical_keywords = [
        'ä¸­å…±ä¸­å¤®', 'å›½åŠ¡é™¢', 'æ”¿æ²»å±€', 'è¯ç›‘ä¼š', 'å¤®è¡Œ', 'äººæ°‘é“¶è¡Œ', 'ä¹ è¿‘å¹³', 'æå¼º', 
        'å°èŠ±ç¨', 'é™å‡†', 'é™æ¯', 'ç¤¾è', 'ä¿¡è´·', 'LPR', 'IPO', 'å¹³å‡†åŸºé‡‘', 'å›½å®¶é˜Ÿ',
        'ä¸­å¤®æ±‡é‡‘', 'å›½æ–°æŠ•èµ„', 'å‘æ”¹å§”', 'ç»Ÿè®¡å±€', 'è´¢æ”¿éƒ¨', 'å·¥ä¿¡éƒ¨', 'å›½èµ„å§”', 'é‡‘èç›‘ç®¡æ€»å±€'
    ]
    for kw in critical_keywords:
        if kw in text: score += 15
            
    # 2. A-Share Themes
    market_keywords = [
        'Aè‚¡', 'è·æ‰¹', 'ä¸­æ ‡', 'å›è´­', 'å¢æŒ', 'åˆ†çº¢', 'ä¸šç»©', 'å‘å¸ƒ', 'å°å‘', 'é€šè¿‡',
        'æ–°èƒ½æº', 'å…‰ä¼', 'åŠå¯¼ä½“', 'èŠ¯ç‰‡', 'äººå·¥æ™ºèƒ½', 'AI', 'ç®—åŠ›', 'åä¸º', 'æˆ¿åœ°äº§', 'æ¥¼å¸‚',
        'ä½ç©ºç»æµ', 'å•†ä¸šèˆªå¤©', 'åŒ»è¯', 'ç™½é…’', 'é“¶è¡Œ', 'åˆ¸å•†', 'ä¿é™©', 'æ±½è½¦', 'ç”µæ± '
    ]
    for kw in market_keywords:
        if kw in text: score += 5

    score += 1 
    return score

def filter_top_news(data, limit=10, is_weekly=False):
    """Select Top N items and Return (List, BullishSectors, BearishSectors)"""
    candidates = []
    unique_titles = set()
    
    for item in data:
        full_text = item['title']
        clean_text = clean_text_gentle(full_text)
        
        # Dedupe
        clean_key = re.sub(r'[^\w]', '', clean_text)[:8]
        if clean_key in unique_titles: continue
        unique_titles.add(clean_key)
        
        score = calculate_importance(full_text)
        
        if score > 0:
            direction, target = get_sentiment_and_target(full_text)
            
            # STRICT FILTER
            if not direction:
                continue
            
            if len(clean_text) < 4: continue
            
            candidates.append({
                'time': item['time'],
                'title': clean_text,
                'score': score,
                'direction': direction,
                'target': target
            })
            
    # Sort
    candidates.sort(key=lambda x: (x['score'], x['time']), reverse=True)
    top_items = candidates[:limit]
    top_items.sort(key=lambda x: x['time'], reverse=True)
    
    formatted_list = []
    
    # Weekday Map
    wd_map = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]
    
    # Net Score Logic
    sector_scores = {}
    
    for item in top_items:
        if is_weekly:
            wd_idx = item['time'].weekday()
            t_str = wd_map[wd_idx]
        else:
            t_str = item['time'].strftime('%H:%M')
            
        # Display Tag: Hide generic targets
        generic_targets = ["è¡Œä¸š", "å®è§‚", "ä¸ªè‚¡"]
        target_display = ""
        if item['target'] and item['target'] not in generic_targets:
            target_display = item['target']
            sem_tag = f"ã€{item['direction']}Â·{item['target']}ã€‘"
        else:
            sem_tag = f"ã€{item['direction']}ã€‘"
        
        # Truncate for brevity (User Request)
        display_title = item['title']
        if len(display_title) > 60:
            display_title = display_title[:58] + "..."
            
        formatted_list.append(f"[{t_str}]{sem_tag} {display_title}")
        
        # Accumulate Net Score for Footer
        if target_display:
            curr = sector_scores.get(target_display, 0)
            if item['direction'] == "åˆ©å¤š":
                sector_scores[target_display] = curr + 1
            elif item['direction'] == "åˆ©ç©º":
                sector_scores[target_display] = curr - 1

    # Separate by Net Score > 0 (Bullish) or < 0 (Bearish)
    bullish_list = [k for k, v in sector_scores.items() if v > 0]
    bearish_list = [k for k, v in sector_scores.items() if v < 0]
    
    return formatted_list, bullish_list, bearish_list

def save_prompt(content, filename, output_dir):
    path = os.path.join(output_dir, "AIæç¤ºè¯", filename)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"Saved: {path}")



def run(date_str, output_dir, run_weekly=False):
    date_disp = datetime.strptime(date_str, '%Y%m%d').strftime('%mæœˆ%dæ—¥')
    
    # 1. Daily Summary
    daily_data = fetch_eastmoney_data(target_window_hours=24)
    daily_top, d_bull, d_bear = filter_top_news(daily_data, limit=10, is_weekly=False)
    
    daily_txt = "\n".join([f"- {news}" for news in daily_top]) or "- [æš‚æ— é‡å¤§Aè‚¡é¢˜ææ¶ˆæ¯]"
    
    d_bull_str = "ã€".join(d_bull) if d_bull else "æ— "
    d_bear_str = "ã€".join(d_bear) if d_bear else "æ— "
    
    daily_content = f"""# Aè‚¡è´¢ç»æ—¥å† 24h - AIç»˜å›¾Prompt ({date_disp})
# æ•°æ®æ¥æº: ä¸œæ–¹è´¢å¯Œ (Top 10 é¢˜æç²¾é€‰)

## å›¾ç‰‡è§„æ ¼
- æ¯”ä¾‹: 9:16 ç«–ç‰ˆ
- é£æ ¼: æ‰‹ç»˜/æ‰‹è´¦é£æ ¼ï¼Œæš–è‰²çº¸å¼ è´¨æ„Ÿ
- èƒŒæ™¯è‰²: #F5E6C8 çº¸é»„è‰²
- é…è‰²: åˆ©å¤š=çº¢è‰², åˆ©ç©º=ç»¿è‰² (ä¸­å›½Aè‚¡çº¢æ¶¨ç»¿è·Œ)

## æ ‡é¢˜
**ğŸ“… Aè‚¡24å°æ—¶é‡è¦èµ„è®¯ç²¾é€‰** (çº¢è‰²)
**A-Share Daily Focus | {date_disp}**

---

## ğŸ‡¨ğŸ‡³ Aè‚¡/é¢˜æ/æ”¿ç­– (Top 10)

{daily_txt}

---

## ğŸ’¡ äº¤æ˜“æé†’
- **åˆ©å¤š** (çº¢è‰²)ï¼š{d_bull_str}
- **åˆ©ç©º** (ç»¿è‰²)ï¼š{d_bear_str}
- æ€»ç»“ä¸æ˜“ï¼Œæ¯å¤©æ”¶ç›˜åæ¨é€ï¼Œç‚¹èµå…³æ³¨ä¸è¿·è·¯ï¼

## AIç»˜å›¾Prompt (English)

Hand-drawn financial infographic poster, China A-share market news, 24h summary {date_disp}.

**Style**: Warm cream paper texture (#F5E6C8), vintage notebook aesthetic, handwritten Chinese fonts.
**Color Coding**: Tags "åˆ©å¤š" MUST be RED. Tags "åˆ©ç©º" MUST be GREEN.

**Layout**:
- Title: "Important Selection" hand-drawn style.
- Section 1: Top 10 News List with Sector Tags.
- Footer: "Like & Follow".
"""
    save_prompt(daily_content, "æ ¸å¿ƒè¦é—»_Prompt.txt", output_dir)

    # --- New: Automate Podcast Text Generation (Daily) ---
    podcast_dir = os.path.join(output_dir, "../podcast_inputs") # Save to daily root podcast_inputs
    os.makedirs(podcast_dir, exist_ok=True)
    podcast_text = generate_podcast_text(daily_top, is_weekly=False)
    podcast_file = os.path.join(podcast_dir, "core_news_daily.txt")
    with open(podcast_file, 'w', encoding='utf-8') as f:
        f.write(podcast_text)
    print(f"ğŸ™ï¸ Podcast text saved to: {podcast_file}")

    # --- New: Automate Image Generation (Daily) ---
    raw_prompt_daily = get_raw_image_prompt_daily(date_disp)
    image_dir = os.path.join(output_dir, "../images") # Save to daily root images
    os.makedirs(image_dir, exist_ok=True)
    image_path_daily = os.path.join(image_dir, "core_news_daily_cover.png")

    print("\nğŸ¨ Generating Daily News Cover Image...")
    generate_image_from_text(raw_prompt_daily, image_path_daily)


    # 2. Weekly Summary
    if run_weekly:
        print("Generating Weekly Summary...")
        weekly_data = fetch_eastmoney_data(target_window_hours=168)
        weekly_top, w_bull, w_bear = filter_top_news(weekly_data, limit=10, is_weekly=True)

        weekly_txt = "\n".join([f"- {news}" for news in weekly_top])
        w_bull_str = "ã€".join(w_bull) if w_bull else "æ— "
        w_bear_str = "ã€".join(w_bear) if w_bear else "æ— "

        weekly_content = f"""# Aè‚¡æœ¬å‘¨æ ¸å¿ƒå›é¡¾ - AIç»˜å›¾Prompt
# æ•°æ®æ¥æº: ä¸œæ–¹è´¢å¯Œ (7å¤© Top 10)

## å›¾ç‰‡è§„æ ¼
- æ¯”ä¾‹: 9:16 ç«–ç‰ˆ
- é£æ ¼: æ‰‹ç»˜/æ‰‹è´¦é£æ ¼ï¼Œæš–è‰²çº¸å¼ è´¨æ„Ÿ
- èƒŒæ™¯è‰²: #F5E6C8 çº¸é»„è‰²
- é…è‰²: åˆ©å¤š=çº¢è‰², åˆ©ç©º=ç»¿è‰² (ä¸­å›½Aè‚¡çº¢æ¶¨ç»¿è·Œ)

## æ ‡é¢˜
**ğŸ“… Aè‚¡æœ¬å‘¨é‡è¦å›é¡¾** (çº¢è‰²)
**A-Share Weekly Review**

---

## ğŸ‡¨ğŸ‡³ æœ¬å‘¨é‡ç£… (Top 10)
> è¿‡å»7å¤© æ”¿ç­–/è¡Œä¸š æ ¸å¿ƒäº‹ä»¶

{weekly_txt}

---

## ğŸ’¡ æŠ•èµ„ç¬”è®°
- **åˆ©å¤š** (çº¢è‰²)ï¼š{w_bull_str}
- **åˆ©ç©º** (ç»¿è‰²)ï¼š{w_bear_str}
- æ€»ç»“ä¸æ˜“ï¼Œæ¯å¤©æ”¶ç›˜åæ¨é€ï¼Œç‚¹èµå…³æ³¨ä¸è¿·è·¯ï¼

## AIç»˜å›¾Prompt (English)

Hand-drawn financial infographic poster, China A-share weekly summary.

**Style**: Warm cream paper texture (#F5E6C8), vintage notebook aesthetic, handwritten Chinese fonts.
**Color Coding**: Tags "åˆ©å¤š" MUST be RED. Tags "åˆ©ç©º" MUST be GREEN.

**Layout**:
- Title: "Weekly Focus" hand-drawn style.
- Section 1: Top 10 Weekly News List.
- Footer: "Like & Follow".
"""
        save_prompt(weekly_content, "å‘¨åˆŠ/æœ¬å‘¨è¦é—»_Prompt.txt", output_dir)

        # --- New: Automate Podcast Text Generation (Weekly) ---
        podcast_text_weekly = generate_podcast_text(weekly_top, is_weekly=True)
        podcast_file_weekly = os.path.join(podcast_dir, "core_news_weekly.txt")
        with open(podcast_file_weekly, 'w', encoding='utf-8') as f:
            f.write(podcast_text_weekly)
        print(f"ğŸ™ï¸ Podcast text saved to: {podcast_file_weekly}")

        # --- New: Automate Image Generation (Weekly) ---
        raw_prompt_weekly = get_raw_image_prompt_weekly()
        image_path_weekly = os.path.join(image_dir, "core_news_weekly_cover.png")

        print("\nğŸ¨ Generating Weekly News Cover Image...")
        generate_image_from_text(raw_prompt_weekly, image_path_weekly)

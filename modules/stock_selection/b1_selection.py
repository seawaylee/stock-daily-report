"""
å…¨å¸‚åœºé€‰è‚¡ + AIæ™ºèƒ½åˆ†æç”ŸæˆæŠ¥å‘Š
1. 300å¹¶å‘å…¨å¸‚åœºé€‰è‚¡ï¼ˆå¸‚å€¼100äº¿+ï¼Œæ’é™¤STï¼‰
2. æ¥å…¥Geminiåˆ†æTop5å€¼åšç‡

"""
import sys
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import requests
import base64
import time
import random
import numpy as np
from tqdm import tqdm

# å¯¼å…¥é…ç½®å’ŒPromptæ¨¡å—
from common.config import MAX_WORKERS, MIN_MARKET_CAP
from common.prompts import (
    NumpyEncoder, 
    get_analysis_prompt, 
    get_image_prompt
)

# å¯¼å…¥æ•°æ®è·å–å’Œä¿¡å·æ£€æµ‹æ¨¡å—

# å¯¼å…¥æ•°æ®è·å–å’Œä¿¡å·æ£€æµ‹æ¨¡å—
from common.data_fetcher import get_all_stock_list, get_stock_data
from common.signals import check_stock_signal
# Import new LLM client
from common.llm_client import chat_completion

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


# ============ çƒ­é—¨é¢˜æè¿‡æ»¤åŠŸèƒ½ ============

def get_hot_sectors_from_fish_basin(date_dir: str, top_n: int = 5):
    """
    ä»è¶‹åŠ¿æ¨¡å‹Promptæ–‡ä»¶ä¸­æå–Top Nçƒ­é—¨é¢˜æ
    
    Args:
        date_dir: æ—¥æœŸç›®å½•ï¼ˆä¾‹å¦‚ï¼šresults/20260204ï¼‰
        top_n: è·å–å‰Nä¸ªé¢˜æï¼Œé»˜è®¤5
    
    Returns:
        é¢˜æåˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š['è´µé‡‘å±', 'æœ‰è‰²é‡‘å±', 'å…‰ä¼è®¾å¤‡', 'çŸ³æ²¹åŠ å·¥è´¸æ˜“', 'åŠå¯¼ä½“']
        å¦‚æœæå–å¤±è´¥è¿”å›ç©ºåˆ—è¡¨
    """
    import re
    from datetime import datetime, timedelta
    
    # å°è¯•è¯»å–ä»Šå¤©çš„è¶‹åŠ¿æ¨¡å‹Promptæ–‡ä»¶
    prompt_file = os.path.join(date_dir, "AIæç¤ºè¯", "è¶‹åŠ¿æ¨¡å‹_åˆå¹¶_Prompt.txt")
    
    # Fallback: å¦‚æœä»Šå¤©çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æ˜¨å¤©çš„
    if not os.path.exists(prompt_file):
        print(f"âš ï¸ ä»Šæ—¥è¶‹åŠ¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")
        # å°è¯•æ˜¨å¤©
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        prompt_file = os.path.join("results", yesterday, "AIæç¤ºè¯", "è¶‹åŠ¿æ¨¡å‹_åˆå¹¶_Prompt.txt")
        if not os.path.exists(prompt_file):
            print(f"âŒ æ˜¨æ—¥è¶‹åŠ¿æ¨¡å‹æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {prompt_file}")
            return []
        else:
            print(f"âœ… ä½¿ç”¨æ˜¨æ—¥è¶‹åŠ¿æ¨¡å‹æ–‡ä»¶: {prompt_file}")
    
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾ "SECTION 2: çƒ­é—¨é¢˜æè¶‹åŠ¿" éƒ¨åˆ†
        section_match = re.search(r'SECTION 2:.*?çƒ­é—¨é¢˜æ.*?\*\*Data\*\*:(.*?)---', content, re.DOTALL)
        if not section_match:
            print("âŒ æœªæ‰¾åˆ°çƒ­é—¨é¢˜ææ•°æ®æ®µ")
            return []
        
        data_section = section_match.group(1)
        
        # æå–é¢˜æåç§°ï¼šæ ¼å¼ä¸º "1. â— è´µé‡‘å± | æ¶¨è·Œ:..."
        # æ­£åˆ™åŒ¹é…ï¼šæ•°å­—. â—‹/â— é¢˜æåç§° |
        pattern = r'\d+\.\s*[â—â—‹]\s*([^\s|]+)\s*\|'
        matches = re.findall(pattern, data_section)
        
        if not matches:
            print("âŒ æœªèƒ½æå–é¢˜æåç§°")
            return []
        
        # å–å‰top_nä¸ª
        hot_sectors = matches[:top_n]
        print(f"ğŸ“Š æå–åˆ°Top{top_n}çƒ­é—¨é¢˜æ: {hot_sectors}")
        return hot_sectors
        
    except Exception as e:
        print(f"âŒ æå–çƒ­é—¨é¢˜æå¤±è´¥: {e}")
        return []


def match_stock_sector(stock_info: dict, hot_sectors: list) -> bool:
    """
    åˆ¤æ–­è‚¡ç¥¨æ˜¯å¦å±äºçƒ­é—¨é¢˜æ
    
    Args:
        stock_info: è‚¡ç¥¨ä¿¡æ¯å­—å…¸ï¼ˆéœ€åŒ…å«'industry'æˆ–'sector'å­—æ®µï¼‰
        hot_sectors: çƒ­é—¨é¢˜æåˆ—è¡¨
    
    Returns:
        æ˜¯å¦åŒ¹é…ä»»ä¸€çƒ­é—¨é¢˜æ
    """
    if not hot_sectors:
        return False
    
    # è·å–è‚¡ç¥¨çš„è¡Œä¸š/é¢˜æä¿¡æ¯
    industry = stock_info.get('industry', '')
    sector = stock_info.get('sector', '')
    combined = f"{industry} {sector}".lower()
    
    # é¢˜ææ˜ å°„è¡¨ï¼šFish Basiné¢˜æå -> å¯èƒ½çš„è¡Œä¸šå…³é”®è¯
    sector_mapping = {
        'è´µé‡‘å±': ['é»„é‡‘', 'ç™½é“¶', 'è´µé‡‘å±'],
        'æœ‰è‰²é‡‘å±': ['æœ‰è‰²', 'é“', 'é“œ', 'é”Œ', 'é•', 'é’´', 'é”‚'],
        'å…‰ä¼è®¾å¤‡': ['å…‰ä¼', 'å¤ªé˜³èƒ½', 'é€†å˜å™¨', 'ç¡…ç‰‡'],
        'çŸ³æ²¹åŠ å·¥è´¸æ˜“': ['çŸ³æ²¹', 'çŸ³åŒ–', 'åŒ–å·¥', 'ç‚¼åŒ–'],
        'åŠå¯¼ä½“': ['åŠå¯¼ä½“', 'èŠ¯ç‰‡', 'é›†æˆç”µè·¯', 'IC', 'æ™¶åœ†'],
        'å•†ä¸šèˆªå¤©': ['èˆªå¤©', 'å«æ˜Ÿ', 'ç«ç®­', 'èˆªç©ºèˆªå¤©'],
        'ä¿é™©': ['ä¿é™©', 'å¯¿é™©', 'è´¢é™©'],
        'ç¨€åœŸ': ['ç¨€åœŸ', 'é’•é“ç¡¼', 'æ°¸ç£'],
        'é€šä¿¡è®¾å¤‡': ['é€šä¿¡', '5G', 'å…‰é€šä¿¡', 'ç½‘ç»œè®¾å¤‡'],
        'ç»†åˆ†åŒ–å·¥': ['åŒ–å·¥', 'åŒ–å­¦', 'ç²¾ç»†åŒ–å·¥'],
        'ç”µç½‘è®¾å¤‡': ['ç”µç½‘', 'ç”µåŠ›è®¾å¤‡', 'ç‰¹é«˜å‹', 'å˜å‹å™¨'],
        'ç…¤ç‚­': ['ç…¤ç‚­', 'ç…¤çŸ¿', 'ç„¦ç…¤'],
        'æˆ¿åœ°äº§': ['æˆ¿åœ°äº§', 'åœ°äº§', 'ç‰©ä¸š'],
        'é£ç”µè®¾å¤‡': ['é£ç”µ', 'é£èƒ½', 'é£æœº'],
        'ç”µåŠ›': ['ç”µåŠ›', 'å‘ç”µ', 'ç«ç”µ', 'æ°´ç”µ'],
        'å…»æ®–': ['å…»æ®–', 'çŒª', 'é¸¡', 'ç¦½'],
        'åŒ»ç–—æœåŠ¡': ['åŒ»ç–—', 'åŒ»é™¢', 'è¯Šæ–­'],
        'æ–°èƒ½æº': ['æ–°èƒ½æº', 'ç”µæ± ', 'å‚¨èƒ½', 'é”‚ç”µ'],
        'äººå·¥æ™ºèƒ½': ['äººå·¥æ™ºèƒ½', 'AI', 'ç®—åŠ›', 'èŠ¯ç‰‡', 'äº‘è®¡ç®—'],
        'æ—…æ¸¸': ['æ—…æ¸¸', 'é…’åº—', 'æ™¯åŒº'],
    }
    
    # åŒ¹é…é€»è¾‘
    for hot_sector in hot_sectors:
        # ç›´æ¥åŒ¹é…
        if hot_sector.lower() in combined:
            return True
        
        # é€šè¿‡æ˜ å°„è¡¨åŒ¹é…
        if hot_sector in sector_mapping:
            keywords = sector_mapping[hot_sector]
            for keyword in keywords:
                if keyword.lower() in combined:
                    return True
    
    return False


def process_single_stock(args):
    """å¤„ç†å•åªè‚¡ç¥¨"""
    code, name, market_cap, industry = args
    # Add random delay to prevent request bursts (Anti-Scraping / Flow Control)
    time.sleep(random.uniform(0.1, 0.2))
    try:
        df = get_stock_data(code, 300)
        if df is None or len(df) < 120:
            return None
        
        result = check_stock_signal(df, code)
        
        # è·å–æœ€åä¸€è¡ŒåŸå§‹æ•°æ®
        last_row = df.iloc[-1].to_dict()
        last_row['date'] = str(last_row['date'])[:10]
        
        return {
            'code': code,
            'name': name,
            'market_cap': float(market_cap),
            'industry': industry if str(industry).lower() != 'nan' else None,  # é¢˜æ/è¡Œä¸š
            'signal': bool(result.get('signal', False)),
            'signals': result.get('signals', []),
            'K': float(result.get('K', 0)),
            'D': float(result.get('D', 0)),
            'J': float(result.get('J', 0)),
            'RSI': float(result.get('RSI', 0)),
            'near_amplitude': float(result.get('è¿‘æœŸæŒ¯å¹…', 0)),
            'far_amplitude': float(result.get('è¿œæœŸæŒ¯å¹…', 0)),
            'raw_data_mock': last_row
        }
    except Exception as e:
        return None


def run_full_selection(force=False):
    """å…¨å¸‚åœºé€‰è‚¡
    
    Args:
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°é€‰è‚¡ï¼Œå¿½ç•¥ä»Šæ—¥å·²æœ‰ç»“æœ
    """
    today_date = datetime.now().strftime('%Y%m%d')
    date_dir = os.path.join("results", today_date)
    os.makedirs(date_dir, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»Šæ—¥ç»“æœ
    if not force:
        import glob
        existing_files = glob.glob(os.path.join(date_dir, f"selected_{today_date}_*.json")) 
        if existing_files:
            latest_file = max(existing_files, key=os.path.getctime)
            print(f"âš¡ å‘ç°ä»Šæ—¥å·²æœ‰é€‰è‚¡ç»“æœ: {latest_file}")
            with open(latest_file, 'r', encoding='utf-8') as f:
                selected = json.load(f)
            # ä»æ–‡ä»¶åæå–å®Œæ•´æ—¶é—´æˆ³ (selected_YYYYMMDD_HHMMSS.json)
            filename = os.path.basename(latest_file)
            # filenameæ ¼å¼: selected_20260118_004725.json
            # å»æ‰å‰ç¼€ selected_ (9 chars) å’Œåç¼€ .json (5 chars)
            timestamp = filename[9:-5] 
            return selected, timestamp
    else:
        print("ğŸ”„ --force æ¨¡å¼ï¼šå¿½ç•¥ä»Šæ—¥ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°é€‰è‚¡")

    print("=" * 70)
    print("  ä¸œæ–¹è´¢å¯Œ - çŸ¥è¡ŒB1é€‰è‚¡ç­–ç•¥ (AIæ™ºèƒ½åˆ†æç‰ˆ)")
    print(f"  å¸‚å€¼ >= {MIN_MARKET_CAP}äº¿ | æ’é™¤ST | å¹¶å‘çº¿ç¨‹: {MAX_WORKERS}")
    print(f"  æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\n[1/4] è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stock_list = get_all_stock_list(min_market_cap=MIN_MARKET_CAP, exclude_st=True)
    
    # æ³¨é‡Šï¼šç§»é™¤500åªè‚¡ç¥¨é™åˆ¶ï¼Œåˆ†æå…¨éƒ¨è‚¡ç¥¨
    # stock_list = stock_list.head(500)
    print(f"âš ï¸ å°†åˆ†æå…¨éƒ¨ {len(stock_list)} åªè‚¡ç¥¨")
    
    if len(stock_list) == 0:
        print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        return [], ""
    
    args_list = [
        (row['code'], row['name'], row['market_cap'], row.get('industry', '')) 
        for _, row in stock_list.iterrows()
    ]
    
    print(f"\n[2/4] å¹¶å‘åˆ†æ {len(args_list)} åªè‚¡ç¥¨çš„ä¿¡å·...")
    
    selected = []
    all_results = []
    
    # ä¿å­˜ç»“æœ
    today_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜æ‰€æœ‰åŸå§‹æ•°æ® (Incremental)
    raw_file = os.path.join(date_dir, f"all_stocks_{today_timestamp}.jsonl")
    print(f"ğŸ“ å®æ—¶æ•°æ®å°†å†™å…¥: {raw_file}")

    # Initial Parallel Fetch
    processed_codes = set()
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_single_stock, args): args[0] for args in args_list}
        
        # Open file in append mode for incremental writing
        with open(raw_file, 'w', encoding='utf-8') as f_out:
            for future in tqdm(as_completed(futures), total=len(futures), desc="é€‰è‚¡è¿›åº¦"):
                result = future.result()
                if result is not None:
                    # Incremental Write
                    f_out.write(json.dumps(result, cls=NumpyEncoder, ensure_ascii=False) + '\n')
                    f_out.flush() # Ensure it flows to disk
                    
                    all_results.append(result)
                    processed_codes.add(result['code'])
                    if result['signal']:
                        selected.append(result)

    # Retry Logic
    missing_args = [arg for arg in args_list if arg[0] not in processed_codes]
    if missing_args:
        print(f"\nğŸ”„ B1 Retry: {len(missing_args)} stocks failed. Retrying sequentially...")
        import time
        
        with open(raw_file, 'a', encoding='utf-8') as f_out:
            for i, args in enumerate(missing_args):
                code = args[0]
                try:
                    # Sequential Retry with delay
                    time.sleep(0.5) 
                    result = process_single_stock(args)
                    
                    if result is not None:
                        f_out.write(json.dumps(result, cls=NumpyEncoder, ensure_ascii=False) + '\n')
                        f_out.flush()
                        all_results.append(result)
                        processed_codes.add(code)
                        if result['signal']:
                            selected.append(result)
                        print(f"   âœ… Retry success: {code}")
                    else:
                        pass # still failed
                except:
                    pass
                
                if (i+1) % 10 == 0:
                    print(f"   Retry progress: {i+1}/{len(missing_args)}")

    # Summary
    success_count = len(processed_codes)
    total_count = len(args_list)
    fail_count = total_count - success_count
    
    print("\n" + "="*40)
    print(f"ğŸ“Š B1é€‰è‚¡ æ‰§è¡Œæ±‡æ€»")
    print(f"âœ… æˆåŠŸ: {success_count}/{total_count}")
    print(f"âŒ å¤±è´¥: {fail_count}/{total_count}")
    print("="*40)
    
    # raw_file is already written incrementally
    
    
    print(f"\nğŸ“ åŸå§‹æ•°æ®: {raw_file} ({len(all_results)} æ¡)")
    
    # ä¿å­˜é€‰è‚¡ç»“æœ
    selected_file = os.path.join(date_dir, f"selected_{today_timestamp}.json")
    with open(selected_file, 'w', encoding='utf-8') as f:
        json.dump(selected, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)
            
    print(f"ğŸ“ é€‰è‚¡ç»“æœ: {selected_file} ({len(selected)} åª)")
    
    return selected, today_timestamp


def save_stock_summary(selected_stocks, date_dir, timestamp):
    """ä¿å­˜ä¾¿äºç§ä¿¡å‘é€çš„æ–‡æœ¬æ±‡æ€»"""
    summary_file = os.path.join(date_dir, f"stock_list_summary_{timestamp}.txt")
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"ğŸ“… é€‰è‚¡æ±‡æ€» {datetime.now().strftime('%Y-%m-%d')}\n")
        f.write(f"ç­–ç•¥: AIå¤§æ¨¡å‹é‡åŒ–\n")
        f.write("-" * 30 + "\n\n")
        
        for idx, stock in enumerate(selected_stocks, 1):
            code = stock['code']
            name = stock['name']
            industry = stock.get('industry', 'æœªçŸ¥è¡Œä¸š')
            
            f.write(f"{idx}. {name} ({code})\n")
            if industry and str(industry).lower() != 'nan':
                f.write(f"   è¡Œä¸š: {industry}\n")
            # è·å–ä¿¡å·åˆ—è¡¨
            stock_signals = stock.get('signals', [])
            
            if stock_signals:
                # ç»Ÿä¸€æœ¯è¯­
                sanitized_signals = [
                    s.replace('B1', 'ä¹°ç‚¹')
                     .replace('B', 'ä¹°ç‚¹')
                     .replace('åŸå§‹', 'æ ‡å‡†')
                    for s in stock_signals
                ]
                f.write(f"   å‘½ä¸­è§„åˆ™: {'+'.join(sanitized_signals)}\n")
            
            f.write("\n")
            
        f.write("-" * 30 + "\n")
        f.write("æ³¨ï¼šæ¬¡æ—¥å…³æ³¨è¿›åœºï¼ŒéæŠ•èµ„å»ºè®®ã€‚\n")
        f.write("æ›´å¤šè¯¦æƒ…è¯·çœ‹æ–‡æ¡ˆåˆ†æã€‚\n")

    print(f"ğŸ“ ç§ä¿¡æ±‡æ€»åˆ—è¡¨: {summary_file}")
    return summary_file

# æ³¨é‡Šï¼šsave_stock_summary åŠŸèƒ½å·²ç§»è‡³ agent_outputs/result_analysis.txt
# æ­¤å‡½æ•°ä¿ç•™ä½†ä¸å†è°ƒç”¨

# ================ è„±æ•å·¥å…·å‡½æ•° ================

def desensitize_stock_name(name):
    """è‚¡ç¥¨åç§°è„±æ•ï¼šä¿ç•™å‰2å­—ï¼Œåé¢æ”¹ä¸ºæ‹¼éŸ³é¦–å­—æ¯å¤§å†™"""
    if len(name) <= 2:
        return name
    
    try:
        from pypinyin import lazy_pinyin, Style
        # è·å–åç¼€çš„æ‹¼éŸ³é¦–å­—æ¯
        suffix = name[2:]
        pinyin_initials = lazy_pinyin(suffix, style=Style.FIRST_LETTER)
        # è½¬å¤§å†™å¹¶æ‹¼æ¥
        initials = ''.join([p.upper() for p in pinyin_initials])
        return name[:2] + initials
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£… pypinyinï¼Œä½¿ç”¨ç®€åŒ–é€»è¾‘
        print("Warning: pypinyin not installed. Using simplified desensitization.")
        suffix = name[2:]
        # å–åç¼€å‰ä¸¤ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
        return name[:2] + (suffix[:2].upper() if len(suffix) >= 2 else suffix.upper())

def desensitize_stock_code(code):
    """è‚¡ç¥¨ä»£ç è„±æ•ï¼šå‰4ä½ä¿ç•™ï¼Œå2ä½æ”¹ä¸º**"""
    if len(code) < 6:
        return code
    return code[:4] + '**'


def normalize_stock_code(code):
    """ç»Ÿä¸€è‚¡ç¥¨ä»£ç ä¸º6ä½æ•°å­—ï¼Œå…¼å®¹ sz000001 / sh600000 / 600000ã€‚"""
    import re

    digits = re.sub(r"\D", "", str(code or ""))
    return digits[-6:] if digits else ""


def compact_reason_text(reason, max_len=18):
    """å‹ç¼©LLMç†ç”±ä¸ºç®€æ´çŸ­å¥ï¼Œé¿å…å¡ç‰‡æ–‡æœ¬è¿‡é•¿ã€‚"""
    import re

    cleaned = re.sub(r"\s+", "", str(reason or "").strip())
    if not cleaned:
        return ""

    parts = re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]", cleaned)
    first = ""
    for part in parts:
        segment = part.strip("ï¼Œ,ï¼š: ")
        if segment:
            first = segment
            break

    if not first:
        first = cleaned

    if len(first) > max_len:
        return first[: max_len - 1] + "â€¦"
    return first


def extract_reason_map_from_analysis(analysis_text):
    """ä»åˆ†ææ–‡æœ¬ä¸­æå– code->ç®€æ´ç†ç”± æ˜ å°„ã€‚"""
    import re

    if not analysis_text:
        return {}

    reason_map = {}
    lines = analysis_text.splitlines()
    current_code = None

    for idx, raw_line in enumerate(lines):
        line = raw_line.strip()
        if not line:
            continue

        code_match = re.search(r"\((\d{6})\)", line)
        if code_match:
            current_code = code_match.group(1)
            inline_reason = re.search(r"æ¨èç†ç”±[:ï¼š]\s*(.+)", line)
            if inline_reason:
                reason_map[current_code] = compact_reason_text(inline_reason.group(1))
                current_code = None
            continue

        if not current_code:
            continue

        reason_line = re.search(r"æ¨èç†ç”±[:ï¼š]?\s*(.*)", line)
        if not reason_line:
            continue

        reason_text = reason_line.group(1).strip(" -*â€¢")
        if not reason_text:
            # "æ¨èç†ç”±ï¼š"å•ç‹¬å ä¸€è¡Œæ—¶ï¼Œå°è¯•è¯»å–ä¸‹ä¸€æ¡éç©ºè¡Œä½œä¸ºç†ç”±ã€‚
            for next_idx in range(idx + 1, min(idx + 4, len(lines))):
                candidate = lines[next_idx].strip(" -*â€¢\t")
                if not candidate:
                    continue
                if re.search(r"\(\d{6}\)", candidate):
                    break
                reason_text = candidate
                break

        compacted = compact_reason_text(reason_text)
        if compacted:
            reason_map[current_code] = compacted
        current_code = None

    return reason_map


def extract_industry_map_from_analysis(analysis_text):
    """ä»åˆ†ææ–‡æœ¬ä¸­æå– code->è¡Œä¸š/é¢˜æ æ˜ å°„ã€‚"""
    import re

    if not analysis_text:
        return {}

    industry_map = {}
    for raw_line in analysis_text.splitlines():
        line = raw_line.strip()
        if not line:
            continue

        match = re.search(r"\((\d{6})\)\s*\|\s*(.+)$", line)
        if not match:
            continue

        code = match.group(1)
        industry = match.group(2).strip()
        industry = re.sub(r"^[\-\*\s]+", "", industry)
        industry = re.sub(r"[\*\s]+$", "", industry)
        industry = re.sub(r"\s{2,}", " ", industry)
        if industry and "æ¨èç†ç”±" not in industry:
            industry_map[code] = industry

    return industry_map


def build_fallback_reason(stock):
    """æ²¡æœ‰LLMç†ç”±æ—¶ï¼Œä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆç®€æ´å›é€€ç†ç”±ã€‚"""
    signal = ",".join(stock.get("signals", []))
    signal = signal.replace("åŸå§‹", "").replace("B1", "ä¹°ç‚¹").replace("B", "ä¹°ç‚¹")
    signal = signal.split(",")[0].strip() or "å½¢æ€å…³æ³¨"

    j_val = round(stock.get("J", 0), 1)
    rsi_val = round(stock.get("RSI", 0), 1)
    return compact_reason_text(f"{signal}ï¼ŒJ{j_val}/RSI{rsi_val}")



def call_gemini_analysis(selected_stocks, date_dir):
    """ä½¿ç”¨LLMç›´æ¥åˆ†æTop5å€¼åšç‡ (å…¨è‡ªåŠ¨æ¨¡å¼)"""
    # å‡†å¤‡åˆ†ææ•°æ®
    stocks_info = []
    for s in selected_stocks:
        raw = s['raw_data_mock']
        stocks_info.append({
            'ä»£ç ': s['code'],
            'åç§°': s['name'],
            'æ€»å¸‚å€¼(äº¿å…ƒ)': round(s['market_cap'], 0),
            'ä¿¡å·ç±»å‹': ', '.join(s['signals']),
            'K': round(s['K'], 1),
            'D': round(s['D'], 1),
            'J': round(s['J'], 1),
            'RSI': round(s['RSI'], 1),
            'è¿‘æœŸæŒ¯å¹…%': round(s['near_amplitude'], 1),
            'è¿œæœŸæŒ¯å¹…%': round(s['far_amplitude'], 1),
            'æ”¶ç›˜ä»·': raw['close'],
            'æˆäº¤é‡': raw['volume']
        })

    # ç»Ÿä¸€ä¸­é—´æ–‡ä»¶ç›®å½•
    temp_dir = os.path.join(date_dir, "temp_data")
    os.makedirs(temp_dir, exist_ok=True)

    prompt = get_analysis_prompt(stocks_info)

    # ä¿å­˜Promptå¤‡ä»½
    task_file = os.path.join(temp_dir, "task_analysis_prompt.txt")
    with open(task_file, 'w', encoding='utf-8') as f:
        f.write(prompt)

    print(f"\n[4/4] æ­£åœ¨è°ƒç”¨ LLM è¿›è¡Œæ·±åº¦åˆ†æ (Top {len(selected_stocks)} åª)...")
    print(f"â³ è¯·æ±‚å·²å‘é€ï¼Œè¯·ç¨å€™...")

    # === ç›´æ¥è°ƒç”¨ LLM ===
    try:
        start_time = time.time()
        analysis_result = chat_completion(prompt, system_prompt="You are a professional quantitative financial analyst.")
        duration = time.time() - start_time

        if not analysis_result:
            print("âŒ LLM åˆ†æå¤±è´¥: è¿”å›ä¸ºç©º")
            return None, prompt

        print(f"âœ… LLM åˆ†æå®Œæˆ (è€—æ—¶ {duration:.1f}s)")

        # ä¿å­˜ç»“æœå¤‡ä»½
        output_file = os.path.join(temp_dir, "result_analysis.txt")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(analysis_result)
        print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")

        # --- æå–å¹¶ä¿å­˜ Top 5 ---
        import re
        # åŒ¹é… (600123) æ ¼å¼
        top_codes = re.findall(r'\((\d{6})\)', analysis_result)
        seen = set()
        unique_codes = []
        for c in top_codes:
            if c not in seen:
                unique_codes.append(c)
                seen.add(c)
        unique_codes = unique_codes[:5]  # Top 5

        stock_map = {}
        for stock in selected_stocks:
            norm_code = normalize_stock_code(stock.get("code"))
            if norm_code and norm_code not in stock_map:
                stock_map[norm_code] = stock
        top_stocks = []
        for c in unique_codes:
            if c in stock_map:
                top_stocks.append(stock_map[c])

        reason_map = extract_reason_map_from_analysis(analysis_result)
        industry_map = extract_industry_map_from_analysis(analysis_result)
        for stock in top_stocks:
            norm_code = normalize_stock_code(stock.get("code"))
            reason = reason_map.get(norm_code)
            if reason:
                stock["reason"] = reason
            industry = industry_map.get(norm_code)
            if industry:
                stock["industry"] = industry

        top5_file = os.path.join(temp_dir, "selected_top5.json")
        with open(top5_file, 'w', encoding='utf-8') as f:
            json.dump(top_stocks, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)

        # å…¼å®¹æ—§æµç¨‹ï¼šä¿ç•™ selected_top10.jsonï¼ˆå†…å®¹åŒTop5ï¼‰
        legacy_top10_file = os.path.join(temp_dir, "selected_top10.json")
        with open(legacy_top10_file, 'w', encoding='utf-8') as f:
            json.dump(top_stocks, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)
        print(f"ğŸ“ å·²æå– Top5 è‚¡ç¥¨æ± : {top5_file} ({len(top_stocks)}åª)")

        return analysis_result, prompt

    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨è¿‡ç¨‹å‡ºé”™: {e}")
        return None, prompt





def generate_image_prompt(gemini_analysis, selected_stocks, date_dir):
    """ç›´æ¥ç”Ÿæˆä¿¡æ¯å›¾æç¤ºè¯ (æ— éœ€AgentäºŒæ¬¡å¤„ç†)"""
    # Import data masking utilities
    from common.data_masking import mask_stock_info

    # å‡†å¤‡è‚¡ç¥¨æ•°æ®æ‘˜è¦(åŒ…å«æŠ€æœ¯æŒ‡æ ‡ + è„±æ•ä¿¡æ¯)
    if len(selected_stocks) > 5:
        print(f"âš ï¸ è­¦å‘Š: ä¼ å…¥å›¾ç‰‡ç”Ÿæˆçš„è‚¡ç¥¨æ•°é‡ä¸º {len(selected_stocks)}ï¼Œæˆªå–Top 5ã€‚")
        selected_stocks = selected_stocks[:5]

    # ä»åˆ†æç»“æœæå– "æ•´ä½“å¸‚åœºå¤ç›˜" å’Œ "æ¬¡æ—¥äº¤æ˜“ç­–ç•¥"
    import re
    
    # --- æå–å¤ç›˜ä¸ç­–ç•¥ ---
    market_review = "æ— å¤ç›˜å†…å®¹"
    tomorrow_strategy = ""
    
    # å°è¯•æå– "Step 4: å›¾ç‰‡ç”Ÿæˆä¸“ç”¨æ‘˜è¦"
    summary_section_match = re.search(r'å›¾ç‰‡ç”Ÿæˆä¸“ç”¨æ‘˜è¦\s*(.+)', gemini_analysis, re.DOTALL)
    
    SUMMARY_FOUND = False
    if summary_section_match:
        summary_content = summary_section_match.group(1)
        
        # æå–å¤ç›˜
        match_rev = re.search(r'ğŸ“\s*(.+?)(?=\n\s*ğŸ’¡|$)', summary_content, re.DOTALL)
        if match_rev:
            extracted_rev = match_rev.group(1).strip().replace('**', '').replace('æ•´ä½“å¤ç›˜', '').strip()
            if extracted_rev:
                market_review = extracted_rev
                SUMMARY_FOUND = True
        
        # æå–ç­–ç•¥
        match_str = re.search(r'ğŸ’¡\s*(.+)', summary_content, re.DOTALL)
        if match_str:
            extracted_str = match_str.group(1).strip().replace('**', '').replace('æ¬¡æ—¥ç­–ç•¥', '').strip()
            if extracted_str:
                tomorrow_strategy = extracted_str
                SUMMARY_FOUND = True
                
    if not SUMMARY_FOUND:
        print("âš ï¸ æœªæ‰¾åˆ°ä¸“ç”¨æ‘˜è¦ï¼Œä½¿ç”¨æ™ºèƒ½æå–å›é€€æ¨¡å¼...")
        # å›é€€æ¨¡å¼ï¼šä»æ­£æ–‡æå–ç¬¬ä¸€æ®µ
        match_review = re.search(r'æ•´ä½“å¸‚åœºå¤ç›˜\s+(.+?)(?=\n\s*æ¬¡æ—¥äº¤æ˜“ç­–ç•¥|$)', gemini_analysis, re.DOTALL)
        if match_review:
            full_review = match_review.group(1).strip()
            market_review = full_review.split('\n')[0].strip().replace('**', '')

        match_strategy = re.search(r'æ¬¡æ—¥äº¤æ˜“ç­–ç•¥\s+(.+?)(?=\n\s*é£é™©æç¤º|$)', gemini_analysis, re.DOTALL)
        if match_strategy:
            full_strategy = match_strategy.group(1).strip()
            # æå– **æ ¸å¿ƒç‚¹**
            strategy_points = re.findall(r'\*\*(.*?)\*\*', full_strategy)
            if strategy_points:
                tomorrow_strategy = "ã€".join(strategy_points[:3])
            else:
                tomorrow_strategy = full_strategy.split('\n')[0].strip().replace('**', '')

    
    # æ„å»ºåŠ¨æ€ Footer å†…å®¹
    footer_content = ""
    # if market_review and market_review != "æ— å¤ç›˜å†…å®¹":
    #    footer_content += f"ğŸ“ æ•´ä½“å¤ç›˜\n{market_review}\n\n"
    
    # if tomorrow_strategy:
    #    footer_content += f"ğŸ’¡ æ¬¡æ—¥ç­–ç•¥\n{tomorrow_strategy}"
    # 
    # USER REQUEST: Specific Footer with Disclaimer
    footer_content = """
**FOOTER:**
"Daily AI Algo Strategy | High Value Ratio Stocks | Follow for Updates"
(Render in Chinese: "æ¯æ—¥ç›˜ååˆ†äº«AIé‡åŒ–ç­–ç•¥çš„é«˜å€¼åšç‡è‚¡ç¥¨ï¼Œç‚¹èµå…³æ³¨ä¸è¿·è·¯")

**å…è´£å£°æ˜ (Disclaimer):**
æœ¬å†…å®¹ä»…ä¾›å­¦ä¹ äº¤æµï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚
è‚¡å¸‚æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚è¯·ç‹¬ç«‹æ€è€ƒï¼Œç†æ€§å†³ç­–ã€‚
"""

    # --- Generate Card Text with Trading Strategy (Python Logic) ---
    reason_map = extract_reason_map_from_analysis(gemini_analysis)
    industry_map = extract_industry_map_from_analysis(gemini_analysis)
    cards_text = ""
    for idx, s in enumerate(selected_stocks, 1):
        # åº”ç”¨æ•°æ®è„±æ•ï¼šä»£ç å2ä½æ›¿æ¢ä¸º**ï¼Œåç§°å2å­—æ›¿æ¢ä¸ºæ‹¼éŸ³ç¼©å†™
        masked_code, masked_name = mask_stock_info(s['code'], s['name'])

        norm_code = normalize_stock_code(s.get("code"))
        industry = s.get('industry', '')
        if not industry or industry == 'æœªçŸ¥' or str(industry).lower() == 'nan':
            industry = industry_map.get(norm_code, "è¡Œä¸šå¾…è¡¥å……")

        signals = ','.join(s.get('signals', [])).replace('B1', 'ä¹°ç‚¹').replace('B', 'ä¹°ç‚¹').replace('åŸå§‹ä¹°ç‚¹', 'ä¹°ç‚¹')
        signals = signals.split(',')[0]  # First signal

        J_val = round(s.get('J', 0), 2)
        RSI_val = round(s.get('RSI', 0), 2)
        llm_reason = s.get("reason") or reason_map.get(norm_code) or build_fallback_reason(s)
        llm_reason = compact_reason_text(llm_reason)

        # ä½¿ç”¨è„±æ•åçš„ä»£ç å’Œåç§°
        line1 = f"#{idx} {masked_name} | {masked_code} | ğŸ­ {industry}"
        line2 = f"ğŸ“Œ é€‰è‚¡ç†ç”±: {llm_reason}"
        line3 = f"ğŸ“Š ä¿¡å·: {signals} | J={J_val} RSI={RSI_val}"

        cards_text += f"{line1}\n{line2}\n{line3}\n\n"

    # --- Final Prompt Construction ---
    final_prompt = f"""(masterpiece, best quality), (vertical:1.2), (aspect ratio: 10:16), (sketch style), (hand drawn), (infographic)

Create a TALL VERTICAL PORTRAIT IMAGE (Aspect Ratio 10:16) HAND-DRAWN SKETCH style stock market infographic poster.

**CRITICAL: VERTICAL PORTRAIT FORMAT (10:16)**
- The image MUST be significantly taller than it is wide (Phone wallpaper style).
- Aspect Ratio: 10:16.
- Canvas Size: 1600x2560.

**CRITICAL: HAND-DRAWN AESTHETIC**
- Use ONLY pencil sketch lines, charcoal shading, ink pen strokes
- Visible paper grain texture throughout
- Line wobbles and imperfections (authentic hand-drawn feel)
- NO digital smoothness, NO vector graphics
- Shading: crosshatching, stippling, charcoal smudges only
- Background: Hand-drawn red-gold gradient with visible pencil strokes


Center: "AIå¤§æ¨¡å‹é‡åŒ–ç­–ç•¥ Â· çƒ­é—¨é¢˜æç­›é€‰" + "{datetime.now().strftime('%Y-%m-%d')}"
**Visual Highlight**: Add a realistic "Red Ink Stamp" (Seal) near the title with text: "æ¬¡æ—¥æ‹©æœºä¹°å…¥"

**NEW: åŸºäºè¶‹åŠ¿æ¨¡å‹Top5çƒ­é—¨é¢˜æç­›é€‰**

5 stock cards in a single column layout:
Background: Pale blue background with paper texture

**COLOR ACCENT GUIDELINES (Avoid monotone):**
- Keep overall palette soft and low-saturation, not neon.
- Use soft AI-cyan accents (e.g., #BFEFFF / #DFF4FF) for small icons like ğŸ“Œ ğŸ“Š ğŸ­.
- Render line icons and separators with light tint + hand-drawn ink texture.
- For key sentences (especially "é€‰è‚¡ç†ç”±"), add a rounded light highlight background.
- Highlight chip colors: pale cyan #EAF7FF or light amber #FFF4E6.
- Keep text readable: dark gray text on light backgrounds.

**VISUAL CONTENT:**
Refined Hand-Drawn Table/Cards:

{cards_text}

{footer_content}

(Note: This prompt is optimized for the 'Nano Banana Pro3' model. Please ensure all details are consistent with high-quality hand-drawn vector art.)
"""

    return final_prompt, final_prompt

def save_reports(gemini_analysis, today):
    """ä¿å­˜æŠ¥å‘Šï¼ˆç®€åŒ–ç‰ˆ - ä»…ä¿å­˜åˆ°agent_outputsï¼‰"""
    # æ³¨é‡Šï¼šå¤–å±‚é‡å¤æ–‡ä»¶å·²ç§»é™¤ï¼Œæ‰€æœ‰ç»“æœé›†ä¸­åœ¨ agent_outputs/
    # æ­¤å‡½æ•°ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œactual saving done in agent workflow
    date_str = today.split('_')[0]
    date_dir = os.path.join("results", date_str)
    
    print(f"ï¿½ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {date_dir}/agent_outputs/")
    print(f"   - result_analysis.txt")

    print(f"   - result_image_prompt.txt")
    
    return None


def save_prompts(prompts_dict, today):
    """ä¿å­˜æç¤ºè¯è®°å½•ï¼ˆå¯é€‰ - ç”¨äºè°ƒè¯•ï¼‰"""
    # æ³¨é‡Šï¼šæ­¤åŠŸèƒ½å¯é€‰ï¼Œæç¤ºè¯å·²åœ¨ agent_tasks/ ä¸­ä¿å­˜
    # ä¿ç•™æ­¤å‡½æ•°ç”¨äºè°ƒè¯•ç›®çš„
def enrich_stocks_from_analysis(selected_stocks, date_dir):
    """ä»åˆ†ææŠ¥å‘Šå›å¡«è¡Œä¸š/é¢˜æ"""
    try:
        print("ğŸ”„ æ­£åœ¨ä»åˆ†ææŠ¥å‘Šå›å¡« [è¡Œä¸š] å’Œ [è„±æ•ä¿¡æ¯]...")
        temp_dir = os.path.join(date_dir, "temp_data")
        analysis_file = os.path.join(temp_dir, "result_analysis.txt")
        if os.path.exists(analysis_file):
            with open(analysis_file, 'r', encoding='utf-8') as f:
                content = f.read()

            industry_map = extract_industry_map_from_analysis(content)

            # å›å¡«åˆ° selected_stocks
            count = 0
            for stock in selected_stocks:
                pure_code = normalize_stock_code(stock.get("code"))
                if pure_code in industry_map:
                    stock['industry'] = industry_map[pure_code]
                    count += 1

            print(f"âœ… æˆåŠŸä»åˆ†ææŠ¥å‘Šå›å¡« {count} æ¡è¡Œä¸šæ•°æ®")

            # ä¿å­˜å›å¡«åçš„ç»“æœåˆ° selected_top5.json
            top5_file = os.path.join(temp_dir, "selected_top5.json")
            with open(top5_file, 'w', encoding='utf-8') as f:
                json.dump(selected_stocks, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)

            # å…¼å®¹æ—§æµç¨‹
            legacy_top10_file = os.path.join(temp_dir, "selected_top10.json")
            with open(legacy_top10_file, 'w', encoding='utf-8') as f:
                json.dump(selected_stocks, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²æ›´æ–° selected_top5.json")

            return True
        else:
            print("âš ï¸ æœªæ‰¾åˆ° result_analysis.txtï¼Œæ— æ³•å›å¡«ä¿¡æ¯")
            return False
    except Exception as e:
        print(f"âš ï¸ å›å¡«ä¿¡æ¯å‡ºé”™: {e}")
        return False





def run(date_dir=None, force=False):
    """
    Main entry point for Daily Stock Selection & AI Analysis.
    
    Args:
        date_dir: è¾“å‡ºç›®å½•
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œå¿½ç•¥ä»Šæ—¥ç¼“å­˜
    """
    # 1. å…¨å¸‚åœºé€‰è‚¡
    # DEBUG: Mock selection to test downstream
    # print("âš ï¸ DEBUG MODE: Using mocked stock list to skip slow selection")
    # today = datetime.now().strftime('%Y%m%d')
    # selected = [
    #     {
    #         'code': 'sz002931', 'name': 'é”‹é¾™è‚¡ä»½', 'price': 10.5, 'reason': 'Debug', 
    #         'market_cap': 20.0, 'signals': ['B1'], 'K': 50, 'D': 50, 'J': 50, 'RSI': 50, 'near_amplitude': 5.0, 'far_amplitude': 10.0,
    #         'raw_data_mock': {'æ”¶ç›˜': 10.5, 'æ¢æ‰‹%': 5.0, 'close': 10.5, 'volume': 100000}
    #     },
    #     {
    #         'code': 'sh603078', 'name': 'æ±ŸåŒ–å¾®', 'price': 20.0, 'reason': 'Debug', 
    #         'market_cap': 30.0, 'signals': ['B1'], 'K': 60, 'D': 60, 'J': 60, 'RSI': 60, 'near_amplitude': 6.0, 'far_amplitude': 12.0,
    #         'raw_data_mock': {'æ”¶ç›˜': 20.0, 'æ¢æ‰‹%': 3.2, 'close': 20.0, 'volume': 200000}
    #     },
    #     {
    #         'code': 'sz000063', 'name': 'ä¸­å…´é€šè®¯', 'price': 30.0, 'reason': 'Debug',
    #         'market_cap': 1000.0, 'signals': ['B1'], 'K': 70, 'D': 70, 'J': 70, 'RSI': 70, 'near_amplitude': 3.0, 'far_amplitude': 8.0,
    #         'raw_data_mock': {'æ”¶ç›˜': 30.0, 'æ¢æ‰‹%': 2.1, 'close': 30.0, 'volume': 500000}
    #     }
    # ]
    selected, today = run_full_selection(force=force)
    
    # ç¡®ä¿æ—¥æœŸæ–‡ä»¶å¤¹å­˜åœ¨
    date_str = today.split('_')[0]
    
    # Use passed in date_dir if provided, otherwise default
    if not date_dir:
        date_dir = os.path.join("results", date_str)
        
    os.makedirs(date_dir, exist_ok=True)
    
    gemini_analysis = None
    
    # 2. **NEW** è·å–çƒ­é—¨é¢˜æå¹¶è¿‡æ»¤è‚¡ç¥¨
    print("\n" + "="*70)
    print("  Step 2: çƒ­é—¨é¢˜æè¿‡æ»¤")
    print("="*70)
    
    if not selected:
        print("âŒ æ²¡æœ‰é€‰å‡ºè‚¡ç¥¨ï¼Œè·³è¿‡ B1 AI åˆ†æï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–æ¨¡å—...")
        return False
    
    print(f"ğŸ“Š B1æŠ€æœ¯ç­›é€‰ç»“æœ: {len(selected)} åªè‚¡ç¥¨")
    
    # è·å–Top5çƒ­é—¨é¢˜æ
    hot_sectors = get_hot_sectors_from_fish_basin(date_dir, top_n=5)
    
    if not hot_sectors:
        print("âš ï¸ æœªèƒ½è·å–çƒ­é—¨é¢˜æï¼Œè·³è¿‡é¢˜æè¿‡æ»¤ï¼Œä½¿ç”¨å…¨éƒ¨B1è‚¡ç¥¨")
        filtered_stocks = selected
    else:
        # æŒ‰é¢˜æè¿‡æ»¤é€»è¾‘ä¼˜åŒ–ï¼šä¼˜å…ˆé€‰é¢˜æï¼Œä¸è¶³åˆ™æŒ‰ä¿¡å·å¼ºåº¦è¡¥é½
        hot_stocks = [s for s in selected if match_stock_sector(s, hot_sectors)]

        print(f"\nâœ… é¢˜æåŒ¹é…å®Œæˆ:")
        print(f"   - çƒ­é—¨é¢˜æTop5: {', '.join(hot_sectors)}")
        print(f"   - åŒ¹é…é¢˜æçš„B1è‚¡ç¥¨: {len(hot_stocks)} åª")

        # è¡¥é½é€»è¾‘ï¼šç¡®ä¿è‡³å°‘æœ‰ 15-20 åªå€™é€‰è‚¡ä¾› AI ç­›é€‰ Top 5
        target_pool_size = 20
        if len(hot_stocks) < target_pool_size:
            # æ’é™¤å·²é€‰ä¸­çš„é¢˜æè‚¡
            remaining = [s for s in selected if s not in hot_stocks]
            # æŒ‰ J å€¼æ’åºï¼ˆè¶Šä½ä»£è¡¨è¶…å–è¶Šä¸¥é‡ï¼Œä¿¡å·é€šå¸¸è¶Šå¼ºï¼‰
            remaining.sort(key=lambda x: x.get('J', 100))

            padding_needed = min(len(remaining), target_pool_size - len(hot_stocks))
            padding_stocks = remaining[:padding_needed]

            filtered_stocks = hot_stocks + padding_stocks
            print(f"   - ä¿¡å·è¡¥é½: é¢˜æè‚¡ {len(hot_stocks)} åª + å¼ºä¿¡å·è¡¥é½ {len(padding_stocks)} åª = æ€»è®¡ {len(filtered_stocks)} åªå€™é€‰")
        else:
            filtered_stocks = hot_stocks
            print(f"   - é¢˜æè‚¡å……è¶³ï¼Œä½¿ç”¨å‰ {len(filtered_stocks)} åªå€™é€‰")
    
    # 3. è°ƒç”¨AIåˆ†æ (ä½¿ç”¨filtered_stocksè€Œä¸æ˜¯selected)
    print("\n" + "="*70)
    print("  Step 3: AIæ™ºèƒ½åˆ†æ")
    print("="*70)
    
    try:
        # ä¼ å…¥è¿‡æ»¤åçš„è‚¡ç¥¨ä¾›Agentåˆ†æ
        # Agentä¼šä»ä¸­é€‰å‡ºTopè¿›è¡Œæ·±åº¦åˆ†æ
        all_stocks = filtered_stocks
        
        # è°ƒç”¨Agentåˆ†æï¼ˆä¼ å…¥é¢˜æè¿‡æ»¤åçš„å€™é€‰ï¼‰
        gemini_analysis, analysis_prompt = call_gemini_analysis(all_stocks, date_dir)

        # å¦‚æœåˆ†æå¤±è´¥
        if gemini_analysis is None:
            print("\nâŒ LLM åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåç»­æŠ¥å‘Š")
            return False

        print("\nâœ… AI æ™ºèƒ½åˆ†ææµç¨‹ç»“æŸ")

        # åŠ è½½ Top 5 ä¸­é—´æ–‡ä»¶ (call_gemini_analysis å·²ç»ç”Ÿæˆ)
        temp_dir = os.path.join(date_dir, "temp_data")
        top5_file = os.path.join(temp_dir, "selected_top5.json")
        legacy_top10_file = os.path.join(temp_dir, "selected_top10.json")
        top_stocks_list = all_stocks # é»˜è®¤
        
        if os.path.exists(top5_file):
             with open(top5_file, 'r', encoding='utf-8') as f:
                top_stocks_list = json.load(f)
             print(f"âš¡ åŠ è½½ Top5è‚¡ç¥¨æ± : {len(top_stocks_list)} åª")
        elif os.path.exists(legacy_top10_file):
             with open(legacy_top10_file, 'r', encoding='utf-8') as f:
                top_stocks_list = json.load(f)
             print(f"âš¡ åŠ è½½å…¼å®¹è‚¡ç¥¨æ± : {len(top_stocks_list)} åª")
        else:
             print("âš ï¸ æœªæ‰¾åˆ° selected_top5.jsonï¼Œå°†ä½¿ç”¨å…¨éƒ¨è¿‡æ»¤åçš„è‚¡ç¥¨")

        # --- æ–°å¢æ­¥éª¤ï¼šä» AIåˆ†ææŠ¥å‘Š (result_analysis.txt) å›å¡« è¡Œä¸š/é¢˜æ ---
        # ç›®çš„ï¼šç›´æ¥ä»åˆ†æç»“æœå›å¡«ä¿¡æ¯
        enrich_stocks_from_analysis(top_stocks_list, date_dir)
        # -------------------------------------------------------------------
    except Exception as e_ai:
         print(f"âš ï¸ AIåˆ†ææ¨¡å—å‡ºé”™: {e_ai}")
         return False

    # (Skip Image Prompt generation if no analysis, logically)
    if gemini_analysis:
        try:
            # ç”Ÿæˆå›¾ç‰‡æç¤ºè¯
            image_prompt, img_gen_prompt = generate_image_prompt(gemini_analysis, top_stocks_list, date_dir)
            if image_prompt is not None:
                print("âœ… å›¾ç‰‡æç¤ºè¯ç”Ÿæˆå®Œæˆ")
                
                # ä¿å­˜ç‹¬ç«‹å›¾ç‰‡æç¤ºè¯æ–‡ä»¶
                prompt_dir = os.path.join(date_dir, "AIæç¤ºè¯")
                os.makedirs(prompt_dir, exist_ok=True)
                img_prompt_file = os.path.join(prompt_dir, "è¶‹åŠ¿B1é€‰è‚¡_Prompt.txt")
                with open(img_prompt_file, 'w', encoding='utf-8') as f:
                    f.write(image_prompt)
                print(f"ğŸ“ å›¾ç‰‡æç¤ºè¯å·²ä¿å­˜: {img_prompt_file}")
        except Exception as e_img:
            print(f"âš ï¸ å›¾ç‰‡æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e_img}")

    # 4. ä¿å­˜æŠ¥å‘Š
    save_reports(gemini_analysis, today)
    return True


if __name__ == "__main__":
    run()
